=========== MPI ====================

**Installation in conda**
	conda create -n mpi mpi4py numpy scipy petsc petsc4py

**Executable**
1. install: python -m pip install pyinstaller
2. python -m PyInstaller test_parallel.py -> create a dist folder
3. dist/test_parallel: mpirun -np 8 --use-hwthread-cpus test_parallel

**Useful links:**
- Introduction to parallel programming with MPI and Python: https://www.youtube.com/watch?v=36nCgG40DJo
- Parallel computing in python using mpi4py: https://research.computing.yale.edu/sites/default/files/files/mpi4py.pdf
- https://nyu-cds.github.io/python-mpi/

--

**To remember:**
- MPI is a collection of functions and macros. Is not a new programming language.
- Most MPI programs are based on Single Program Multiple Data (SPMD) model. This means that the 
  same executable in a number of processes, but the input data makes each copy compute different things
- Processes (processors) do not share memory, each has a distict memory space. Hence, to 
  communicate data between processes, explicit function calls must be made in the program.
  Atention: For the communication to be successful, both 'Send' and 'Receive' must be executed.

--

**Basic functions:**
- send( self, obj, int dest, int tag = 0 )
	obj: actual object to be sending. Int, float, array
	dest: rank of the process which is the destination for the message
	tag: a tag number which can be used to distinguish among messages

- recv( self, buf = None, int source = ANY_SOURCE, int tag = ANY_TAG, Status status = None )
	obg: object containing the data to be received
	source: rank of the process from which to receive message
	tag: number, which can be used to distinguish among messages
	status: information about the data received, e.g. rank of source, tag, error code
  Fully blocking -> The program stops until the message is actually receive 

-	get_rank( self ): return the rank of this process in a communicator
- get_size( self ): return the number of processes in a communicator

**Collective communications**
- Involve all the processes in a communicator: Most common are broadcast and reduce

- bcast( self, obj, int root = 0 )
	Used when one process has a value and must be propagated among all processers
	obj: object being broadcasted, must be defined on root process before bcast called
	root: rank of broadcast root processes
	must be called on each process

- reduce( self, sendobj, op=SUM, int root = 0 )
	Data from all processes are combined using a binary operation
	sendobj: object to send
	op: reduce operation - MPI.SUM, MPI.MIN
	root: rank of root process
	returns the result of the reduce operation
	must be called in ALL processes in a communicator, but result only available in root process

- gather( self, obj, root = 0 )
	Collect data in ith-process and allocate in process 0 as the ith component of a vector 
	process 0: x0
	process 1: x1 -|
	process 2: x2 ---|
	process 3: x3 ------|

- scatter
	Inverse operation of gather. Distribute a vector in root process, sending the ith component to the ith-process

**Other functions for nonblocking**
- sendrecv( self, obj, sendtag = 0, int dest=0, int source=0 )
	It performs both send and receive such that if no buffering is available, no deadlock will occur

- bsend
	MPI allows the programmer to provide a buffer into which data can be placed until it is delivered

- Isend( obj, dest = 0 ); Irecv( obj, source = 0 )
	comm.Isend( object, dest = 0 )
	req = comm.Isend( object, source = 0 )
	req.Wait()



=========== PETSc ====================
From: PETSc Tutorial - Texas advanced computing center - https://www.youtube.com/watch?v=WVdH520y5ZQ
			Prof. Victor Eijkhout

Routines:
	https://www.mcs.anl.gov/petsc/petsc4py-current/docs/apiref/petsc4py.PETSc.Mat-class.html
	https://www.mcs.anl.gov/petsc/petsc4py-current/docs/apiref/petsc4py.PETSc.Mat-class.html

	Program parameters: Read as commandline argument
	nlocal = PETSc.Options().getInt("n", default_value )

	Calls: Everything in PETSc is an object, with create and destroy calls

	Create vectors:
		comm = PETSc.COMM_WORLD
		x = PETSc.Vec().create( comm = comm )
		x.setType( PETSc.Vec.Type.MPI )

		setValues( index, value ) -> Using global indexing, so proc0 can modify each entry!!
		x.setValue( index, value )
		x.setValues( indexes, values ) -> x.setValues( [1,10],[0.1,0.4] )

	Create matrices:
		For matrix creation, PETSc do not assume any sparsity pattern.
		As any processor can set any element, this could imply lots of malloc calls.
		Strategy: Indicate the matrix sparsity structure by duplicating the construction loop
			once counting, once making
			How many non-zeros per row?
			By default, once indicated the nnz elements there is no possible to create aditional ones!!

		SeqAIJ: Single processor - sparse matrix
			nz: Number of nonzeros per row (or slightly overestimated)
			nnz: array of row lengths (or overestimate) - better for unbalanced matrices

		Iterative solvers:
			Solving a linear system Ax = b with Gaussian elimination can take lots of time/memory
			Alternative: iterative solvers use successive approximations of the solution:
				- Convergence not always guaranteed
				- Possible much faster/less memory
				- Basic operation: y <- Ax executed once per iteration
				- Also needed: preconditioner B \approx A^{-1}

			KSP: linear system solver object
				- ksp.setOperators( pA )
				- ksp.solve( rhs, sol )
				- Program call: kspSetType( KSPGMRES )
				- Command line: kpsSetFromOptions( solver )
					then options -ksp... are parsed: -ksp_type gmres -ksp_gmres_restart 20 -ksp_view...
					Preconditioners: -pctype ( none:Id matrix, jacobi: diag(A), ilu - only sequential, asm: adaptive schwarz, bjacobi, sor, ...) -pc_factor_levels 3

		mpirun -np 10 --oversubscribe --use-hwthread-cpus python3 ksp.py -n 200 -unsymmetry 5 -ksp_view -ksp_monitor -ksp_type gmres -ksp_gmres_restart 200 -pc_type asm -sub_pc_factor_levels 5 -log_view

		check: setValuesCSR( I,J,V, addv=None )